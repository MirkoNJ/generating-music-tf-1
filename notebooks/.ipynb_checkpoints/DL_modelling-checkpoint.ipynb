{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import pdb\n",
    "import random\n",
    "import time\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third party imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.nikhil.midi_related as midi\n",
    "import modules.nikhil.batch as batch\n",
    "\n",
    "from modules.nikhil.MyFunctions import (\n",
    "    alignXy,\n",
    "    Conditional_Probability_Layer,\n",
    "    Input_Kernel, \n",
    "    getNumberOfBatches,\n",
    "    Loss_Function_1,\n",
    "    Loss_Function_2,\n",
    "    LSTM_Cell,\n",
    "    LSTM_Layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extensions and autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting relative directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Working_Directory = os.getcwd()\n",
    "Project_Directory = os.path.abspath(os.path.join(Working_Directory,'..'))\n",
    "Music_In_Directory = Project_Directory + \"/data/\" \n",
    "Output_Directory = Project_Directory + \"/outputs/\"\n",
    "Model_Directory = Output_Directory + \"models/\"\n",
    "Music_Out_Directory = Output_Directory + \"midi/\"\n",
    "Music_Out_Train_Directory = Music_Out_Directory + \"train/\"\n",
    "Checkpoint_Directory = Model_Directory + \"ckpt/\"\n",
    "Numpy_Directory = Model_Directory + \"np/\"\n",
    "\n",
    "Midi_Directories = [\n",
    "    \"albeniz\", \n",
    "    \"beeth\",\n",
    "    \"borodin\",\n",
    "    \"brahms\",\n",
    "    \"burgm\",\n",
    "    \"chopin\", \n",
    "    \"chopin_midi\",\n",
    "    \"debussy\", \n",
    "    \"granados\", \n",
    "    \"grieg\", \n",
    "    \"haydn\", \n",
    "    \"liszt\", \n",
    "    \"mendelssohn\", \n",
    "    \"mozart\", \n",
    "    \"muss\", \n",
    "    \"schubert\", \n",
    "    \"schumann\", \n",
    "    \"tschai\"\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pieces (i.e. import midi files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checkt that importing single midi (i.e. Beethoven's Fuer Elise) works\n",
    "elise = midi.midiToNoteStateMatrix(Music_In_Directory + \"beeth/elise.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded alb_esp1\n",
      "Loaded alb_esp2\n",
      "Loaded alb_esp3\n",
      "Loaded alb_esp4\n",
      "Loaded alb_esp5\n",
      "Loaded alb_se1\n",
      "Loaded alb_se2\n",
      "Loaded alb_se3\n",
      "Loaded alb_se4\n",
      "Loaded alb_se5\n",
      "Loaded alb_se6\n",
      "Loaded alb_se7\n",
      "Loaded alb_se8\n",
      "Loading directory albeniz took 11.888s\n",
      "Loaded appass_1\n",
      "Loaded appass_2\n",
      "Loaded appass_3\n",
      "Loaded beethoven_hammerklavier_1\n",
      "Loaded beethoven_hammerklavier_2\n",
      "Loaded beethoven_hammerklavier_3\n",
      "Loaded beethoven_hammerklavier_4\n",
      "Loaded beethoven_les_adieux_1\n",
      "Loaded beethoven_les_adieux_2\n",
      "Loaded beethoven_les_adieux_3\n",
      "Loaded beethoven_opus10_1\n",
      "Loaded beethoven_opus10_2\n",
      "Loaded beethoven_opus10_3\n",
      "Loaded beethoven_opus22_1\n",
      "Loaded beethoven_opus22_2\n",
      "Loaded beethoven_opus22_3\n",
      "Loaded beethoven_opus22_4\n",
      "Loaded beethoven_opus90_1\n",
      "Loaded beethoven_opus90_2\n",
      "Loaded elise\n",
      "Loaded mond_1\n",
      "Loaded mond_2\n",
      "Loaded mond_3\n",
      "Loaded pathetique_1\n",
      "Loaded pathetique_2\n",
      "Loaded pathetique_3\n",
      "Loaded waldstein_1\n",
      "Loaded waldstein_2\n",
      "Loaded waldstein_3\n",
      "Loading directory beeth took 73.984s\n",
      "Loaded bor_ps1\n",
      "Loaded bor_ps2\n",
      "Loaded bor_ps3\n",
      "Loaded bor_ps4\n",
      "Loaded bor_ps5\n",
      "Loaded bor_ps6\n",
      "Loaded bor_ps7\n",
      "Loading directory borodin took 4.432s\n",
      "Loaded BR_IM6\n",
      "Loaded br_im2\n",
      "Loaded br_im5\n",
      "Loaded br_rhap\n",
      "Loaded brahms_opus117_1\n",
      "Loaded brahms_opus117_2\n",
      "Loaded brahms_opus1_1\n",
      "Loaded brahms_opus1_2\n",
      "Loaded brahms_opus1_3\n",
      "Loaded brahms_opus1_4\n",
      "Loading directory brahms took 23.2s\n",
      "Loaded burg_agitato\n",
      "Loaded burg_erwachen\n",
      "Loaded burg_geschwindigkeit\n",
      "Loaded burg_gewitter\n",
      "Loaded burg_perlen\n",
      "Loaded burg_quelle\n",
      "Loaded burg_spinnerlied\n",
      "Loaded burg_sylphen\n",
      "Loaded burg_trennung\n",
      "Loading directory burgm took 2.73s\n",
      "Loaded chpn-p1\n",
      "Loaded chpn-p10\n",
      "Loaded chpn-p11\n",
      "Loaded chpn-p12\n",
      "Loaded chpn-p13\n",
      "Loaded chpn-p14\n",
      "Loaded chpn-p15\n",
      "Loaded chpn-p16\n",
      "Loaded chpn-p17\n",
      "Loaded chpn-p18\n",
      "Loaded chpn-p19\n",
      "Loaded chpn-p2\n",
      "Loaded chpn-p20\n",
      "Loaded chpn-p21\n",
      "Loaded chpn-p22\n",
      "Loaded chpn-p23\n",
      "Loaded chpn-p24\n",
      "Loaded chpn-p3\n",
      "Loaded chpn-p4\n",
      "Loaded chpn-p5\n",
      "Loaded chpn-p6\n",
      "Loaded chpn-p7\n",
      "Loaded chpn-p8\n",
      "Loaded chpn-p9\n",
      "Loaded chpn_op18\n",
      "Loaded chpn_op23\n",
      "Loaded chpn_op25_e1\n",
      "Loaded chpn_op25_e11\n",
      "Loaded chpn_op25_e12\n",
      "Loaded chpn_op25_e2\n",
      "Loaded chpn_op25_e3\n",
      "Loaded chpn_op25_e4\n",
      "Loaded chpn_op27_1\n",
      "Loaded chpn_op27_2\n",
      "Loaded chpn_op33_2\n",
      "Loaded chpn_op33_4\n",
      "Loaded chpn_op35_1\n",
      "Loaded chpn_op35_2\n",
      "Loaded chpn_op35_3\n",
      "Loaded chpn_op35_4\n",
      "Loaded chpn_op53\n",
      "Loaded chpn_op66\n",
      "Loaded chpn_op7_1\n",
      "Loaded chpn_op7_2\n",
      "Loading directory chopin took 38.121s\n",
      "Loaded chop0601\n",
      "Loaded chop0602\n",
      "Loaded chop0603\n",
      "Loaded chop0701\n",
      "Loaded chop0702\n",
      "Loaded chop0901\n",
      "Loaded chop0902\n",
      "Loaded chop0903\n",
      "Loaded chop1001\n",
      "Loaded chop1002\n",
      "Loaded chop1003\n",
      "Loaded chop1004\n",
      "Loaded chop1005\n",
      "Loaded chop1007\n",
      "Loaded chop1012\n",
      "Loaded chop1501\n",
      "Loaded chop1502\n",
      "Loaded chop1503\n",
      "Loaded chop1800\n",
      "Loaded chop2300\n",
      "Loaded chop2501\n",
      "Loaded chop2502\n",
      "Loaded chop2503\n",
      "Loaded chop2504\n",
      "Loaded chop2506\n",
      "Loaded chop2507\n",
      "Loaded chop2511\n",
      "Loaded chop2512\n",
      "Loaded chop2701\n",
      "Loaded chop2702\n",
      "Loaded chop2801\n",
      "Loaded chop2802\n",
      "Loaded chop2803\n",
      "Loaded chop2804\n",
      "Loaded chop2805\n",
      "Loaded chop2806\n",
      "Loaded chop2807\n",
      "Loaded chop2808\n",
      "Loaded chop2809\n",
      "Loaded chop2810\n",
      "Loaded chop2811\n",
      "Loaded chop2812\n",
      "Loaded chop2813\n",
      "Loaded chop2814\n",
      "Loaded chop2815\n",
      "Loaded chop2816\n",
      "Loaded chop2817\n",
      "Loaded chop2818\n",
      "Loaded chop2819\n",
      "Loaded chop2820\n",
      "Loaded chop2821\n",
      "Loaded chop2822\n",
      "Loaded chop2823\n",
      "Loaded chop2824\n",
      "Loaded chop3002\n",
      "Loaded chop3100\n",
      "Loaded chop3202\n",
      "Loaded chop3302\n",
      "Loaded chop3304\n",
      "Loaded chop3402\n",
      "Loaded chop3502\n",
      "Loaded chop3503\n",
      "Loaded chop3504\n",
      "Loaded chop3701\n",
      "Loaded chop3702\n",
      "Loaded chop3800\n",
      "Loaded chop4001\n",
      "Loaded chop4002\n",
      "Loaded chop4200\n",
      "Loaded chop4700\n",
      "Loaded chop4801\n",
      "Loaded chop4802\n",
      "Loaded chop5200\n",
      "Loaded chop5300\n",
      "Loaded chop5501\n",
      "Loaded chop5502\n",
      "Loaded chop5801\n",
      "Loaded chop5802\n",
      "Loaded chop5803\n",
      "Loaded chop5804\n",
      "Loaded chop5902\n",
      "Loaded chop6201\n",
      "Loaded chop6202\n",
      "Loaded chop6303\n",
      "Loaded chop6401\n",
      "Loaded chop6402\n",
      "Loaded chop6600\n",
      "Loaded chop6701\n",
      "Loaded chop6702\n",
      "Loaded chop6704\n",
      "Loaded chop6802\n",
      "Loaded chop6901\n",
      "Loaded chop6902\n",
      "Loaded chop7002\n",
      "Loaded chop7201\n",
      "Loading directory chopin_midi took 83.499s\n",
      "Loaded DEB_CLAI\n",
      "Loaded DEB_PASS\n",
      "Loaded deb_menu\n",
      "Loaded deb_prel\n",
      "Loaded debussy_cc_1\n",
      "Loaded debussy_cc_2\n",
      "Loaded debussy_cc_3\n",
      "Loaded debussy_cc_4\n",
      "Loaded debussy_cc_6\n",
      "Loading directory debussy took 17.079s\n",
      "Loaded gra_esp_2\n",
      "Loaded gra_esp_3\n",
      "Loaded gra_esp_4\n",
      "Loading directory granados took 3.28s\n",
      "Loaded grieg_album\n",
      "Loaded grieg_berceuse\n",
      "Loaded grieg_brooklet\n",
      "Loaded grieg_butterfly\n",
      "Loaded grieg_elfentanz\n",
      "Loaded grieg_halling\n",
      "Loaded grieg_kobold\n",
      "Loaded grieg_march\n",
      "Loaded grieg_once_upon_a_time\n",
      "Loaded grieg_spring\n",
      "Loaded grieg_voeglein\n",
      "Loaded grieg_waechter\n",
      "Loaded grieg_walzer\n",
      "Loaded grieg_wanderer\n",
      "Loaded grieg_wedding\n",
      "Loaded grieg_zwerge\n",
      "Loading directory grieg took 11.81s\n",
      "Loaded hay_40_1\n",
      "Loaded hay_40_2\n",
      "Loaded haydn_33_1\n",
      "Loaded haydn_33_2\n",
      "Loaded haydn_33_3\n",
      "Loaded haydn_35_1\n",
      "Loaded haydn_35_2\n",
      "Loaded haydn_35_3\n",
      "Loaded haydn_43_1\n",
      "Loaded haydn_43_2\n",
      "Loaded haydn_43_3\n",
      "Loaded haydn_7_1\n",
      "Loaded haydn_7_2\n",
      "Loaded haydn_7_3\n",
      "Loaded haydn_8_1\n",
      "Loaded haydn_8_2\n",
      "Loaded haydn_8_3\n",
      "Loaded haydn_8_4\n",
      "Loaded haydn_9_1\n",
      "Loaded haydn_9_2\n",
      "Loaded haydn_9_3\n",
      "Loading directory haydn took 21.508s\n",
      "Loaded liz_donjuan\n",
      "Loaded liz_et1\n",
      "Loaded liz_et2\n",
      "Loaded liz_et3\n",
      "Loaded liz_et4\n",
      "Loaded liz_et5\n",
      "Loaded liz_et_trans4\n",
      "Loaded liz_et_trans5\n",
      "Loaded liz_et_trans8\n",
      "Loaded liz_liebestraum\n",
      "Loaded liz_rhap02\n",
      "Loaded liz_rhap09\n",
      "Loaded liz_rhap12\n",
      "Loaded liz_rhap15\n",
      "Loading directory liszt took 25.861s\n",
      "Loaded mendel_op19_1\n",
      "Loaded mendel_op19_2\n",
      "Loaded mendel_op19_3\n",
      "Loaded mendel_op19_4\n",
      "Loaded mendel_op19_5\n",
      "Loaded mendel_op19_6\n",
      "Loaded mendel_op30_1\n",
      "Loaded mendel_op30_2\n",
      "Loaded mendel_op30_3\n",
      "Loaded mendel_op30_4\n",
      "Loaded mendel_op30_5\n",
      "Loaded mendel_op53_5\n",
      "Loaded mendel_op62_3\n",
      "Loaded mendel_op62_4\n",
      "Loaded mendel_op62_5\n",
      "Loading directory mendelssohn took 8.566s\n",
      "Loaded mz_311_1\n",
      "Loaded mz_311_2\n",
      "Loaded mz_311_3\n",
      "Loaded mz_330_1\n",
      "Loaded mz_330_2\n",
      "Loaded mz_330_3\n",
      "Loaded mz_331_1\n",
      "Loaded mz_331_2\n",
      "Loaded mz_331_3\n",
      "Loaded mz_332_1\n",
      "Loaded mz_332_2\n",
      "Loaded mz_332_3\n",
      "Loaded mz_333_1\n",
      "Loaded mz_333_2\n",
      "Loaded mz_333_3\n",
      "Loaded mz_545_1\n",
      "Loaded mz_545_2\n",
      "Loaded mz_545_3\n",
      "Loaded mz_570_1\n",
      "Loaded mz_570_2\n",
      "Loaded mz_570_3\n",
      "Loading directory mozart took 54.237s\n",
      "Loaded muss_4\n",
      "Loaded muss_6\n",
      "Loaded muss_8\n",
      "Loading directory muss took 15.21s\n",
      "Loaded schu_143_1\n",
      "Loaded schu_143_2\n",
      "Loaded schu_143_3\n",
      "Loaded schub_d760_1\n",
      "Loaded schub_d760_2\n",
      "Loaded schub_d760_3\n",
      "Loaded schub_d760_4\n",
      "Loaded schub_d960_1\n",
      "Loaded schub_d960_2\n",
      "Loaded schub_d960_3\n",
      "Loaded schub_d960_4\n",
      "Loaded schubert_D850_1\n",
      "Loaded schubert_D850_2\n",
      "Loaded schubert_D850_3\n",
      "Loaded schubert_D850_4\n",
      "Loaded schubert_D935_1\n",
      "Loaded schubert_D935_2\n",
      "Loaded schubert_D935_3\n",
      "Loaded schubert_D935_4\n",
      "Loaded schuim-1\n",
      "Loaded schuim-2\n",
      "Loaded schuim-3\n",
      "Loaded schuim-4\n",
      "Loaded schumm-1\n",
      "Loaded schumm-2\n",
      "Loaded schumm-3\n",
      "Loaded schumm-4\n",
      "Loaded schumm-5\n",
      "Loaded schumm-6\n",
      "Loading directory schubert took 77.426s\n",
      "Loaded schum_abegg\n",
      "Loaded scn15_1\n",
      "Loaded scn15_10\n",
      "Loaded scn15_11\n",
      "Loaded scn15_12\n",
      "Loaded scn15_13\n",
      "Loaded scn15_2\n",
      "Loaded scn15_3\n",
      "Loaded scn15_4\n",
      "Loaded scn15_5\n",
      "Loaded scn15_6\n",
      "Loaded scn15_7\n",
      "Loaded scn15_8\n",
      "Loaded scn15_9\n",
      "Loaded scn16_1\n",
      "Loaded scn16_2\n",
      "Loaded scn16_3\n",
      "Loaded scn16_4\n",
      "Loaded scn16_5\n",
      "Loaded scn16_6\n",
      "Loaded scn16_7\n",
      "Loaded scn16_8\n",
      "Loaded scn68_10\n",
      "Loaded scn68_12\n",
      "Loading directory schumann took 11.728s\n",
      "Loaded ty_april\n",
      "Loaded ty_august\n",
      "Loaded ty_dezember\n",
      "Loaded ty_februar\n",
      "Loaded ty_januar\n",
      "Loaded ty_juli\n",
      "Loaded ty_juni\n",
      "Loaded ty_maerz\n",
      "Loaded ty_mai\n",
      "Loaded ty_november\n",
      "Loaded ty_oktober\n",
      "Loaded ty_september\n",
      "Loading directory tschai took 23.653s\n",
      "Number of total pieces =  374\n",
      "Loading all pieces took 508.212s\n"
     ]
    }
   ],
   "source": [
    "# Import all Midi data\n",
    "min_time_steps = 128 # only files with at least this many 48th note steps are saved\n",
    "lowerBound = 21\n",
    "upperBound = 109\n",
    "\n",
    "all_pieces = {}\n",
    "chopin_only_pieces = {}\n",
    "piano_midi_only_pieces = {}\n",
    "\n",
    "start_time_loading = time.time()\n",
    "time_loading_old = start_time_loading\n",
    "\n",
    "# Gather the pieces from the specified directory\n",
    "for f in range(len(Midi_Directories)):\n",
    "    Training_Midi_Folder = Music_In_Directory + Midi_Directories[f]\n",
    "    if Midi_Directories[f] == 'chopin_midi':\n",
    "        chopin_only_pieces = {**chopin_only_pieces, **midi.loadPieces(Training_Midi_Folder,\n",
    "                                                                      min_time_steps,\n",
    "                                                                      lowerBound, \n",
    "                                                                      upperBound,\n",
    "                                                                      verbose=False,\n",
    "                                                                      verbose_name=True)}\n",
    "    else: \n",
    "        piano_midi_only_pieces = {**piano_midi_only_pieces, **midi.loadPieces(Training_Midi_Folder,\n",
    "                                                                              min_time_steps,\n",
    "                                                                              lowerBound, \n",
    "                                                                              upperBound,\n",
    "                                                                              verbose=False,\n",
    "                                                                              verbose_name=True)}\n",
    "    time_loading_new = time.time()\n",
    "    duration = time_loading_new - time_loading_old\n",
    "    time_loading_old = time_loading_new\n",
    "    print('Loading directory ' + Midi_Directories[f] + ' took ' + str(round(duration, 3)) + 's' )\n",
    "\n",
    "all_pieces = {**chopin_only_pieces, **piano_midi_only_pieces}\n",
    "end_time_loading = time.time()\n",
    "\n",
    "print('Number of total pieces = ', len(all_pieces))    \n",
    "print('Loading all pieces took ' +  str(round(end_time_loading - start_time_loading, 3)) + 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_time_signatures = [16]\n",
    "odd_time_signatures = [12]\n",
    "sel_time_signautres = even_time_signatures\n",
    "#sel_time_signautres = odd_time_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  'incorrect' Chopin pieces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chopin pieces loaded =  95\n"
     ]
    }
   ],
   "source": [
    "print('Number of Chopin pieces loaded = ', len(chopin_only_pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 1, 6: 3, 8: 2, 12: 37, 16: 31, 17: 1, 19: 1, 24: 6, 28: 1, 36: 1, 40: 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check time signature occurences\n",
    "time_signatures = []\n",
    "verbose = False #True\n",
    "keys = list(chopin_only_pieces.keys())\n",
    "for k in keys:\n",
    "    piece = chopin_only_pieces[str(k)]\n",
    "    time_signature = max([b[0][3] for b in  piece])\n",
    "    if str(k) in midi.EXACT_FILES:\n",
    "        time_signatures.append(time_signature)\n",
    "    if verbose:\n",
    "        print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))\n",
    "time_signatures = np.array(time_signatures)\n",
    "unique, counts = np.unique(time_signatures, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: 31}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only include pieces which were not recoreded (i.e. which MIDI files are exact) and are in (2/4 and 4/4) or in (3/4)\n",
    "time_signatures = []\n",
    "verbose = False #True\n",
    "chopin_pieces_filtered = chopin_only_pieces.copy()\n",
    "keys = list(chopin_pieces_filtered.keys())\n",
    "\n",
    "for k in keys:\n",
    "        piece = chopin_pieces_filtered[str(k)]\n",
    "        time_signature = max([b[0][3] for b in  piece])\n",
    "        if not ((time_signature in sel_time_signautres) and str(k) in midi.EXACT_FILES) :\n",
    "            chopin_pieces_filtered.pop(k)\n",
    "        else:\n",
    "            time_signatures.append(time_signature)\n",
    "        if verbose:\n",
    "            print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))\n",
    "time_signatures = np.array(time_signatures)\n",
    "unique, counts = np.unique(time_signatures, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pieces by Chopin left after filtering =  31\n"
     ]
    }
   ],
   "source": [
    "print('Number of pieces by Chopin left after filtering = ', len(chopin_pieces_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Piano Midi pieces by 3/4 or 4/4 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Piano Midi pieces loaded =  279\n"
     ]
    }
   ],
   "source": [
    "print('Number of Piano Midi pieces loaded = ', len(piano_midi_only_pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 2,\n",
       " 6: 16,\n",
       " 8: 47,\n",
       " 9: 1,\n",
       " 12: 92,\n",
       " 16: 89,\n",
       " 18: 6,\n",
       " 24: 10,\n",
       " 28: 1,\n",
       " 32: 6,\n",
       " 36: 3,\n",
       " 40: 4,\n",
       " 48: 1,\n",
       " 56: 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check time signature occurences\n",
    "time_signatures = []\n",
    "verbose = False #True\n",
    "keys = list(piano_midi_only_pieces.keys())\n",
    "for k in keys:\n",
    "    piece = piano_midi_only_pieces[str(k)]\n",
    "    time_signature = max([b[0][3] for b in  piece])\n",
    "    time_signatures.append(time_signature)\n",
    "    if verbose:\n",
    "        print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))\n",
    "time_signatures = np.array(time_signatures)\n",
    "unique, counts = np.unique(time_signatures, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter training pieces by selected time signature\n",
    "piano_midi_pieces_filtered = piano_midi_only_pieces.copy()\n",
    "piano_midi_pieces_remaining = piano_midi_only_pieces.copy()\n",
    "keys = list(piano_midi_only_pieces.keys())\n",
    "verbose = False\n",
    "\n",
    "for k in keys:\n",
    "    piece = piano_midi_only_pieces[str(k)]\n",
    "    time_signature = int(max([b[0][3] for b in  piece]))\n",
    "    if not (time_signature in sel_time_signautres):\n",
    "        piano_midi_pieces_filtered.pop(k)\n",
    "    else:\n",
    "        piano_midi_pieces_remaining.pop(k)\n",
    "    if verbose:\n",
    "        print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Piano Midi pieces left after filtering =  89\n"
     ]
    }
   ],
   "source": [
    "print('Number of Piano Midi pieces left after filtering = ', len(piano_midi_pieces_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Validation pieces split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pieces relevant for training and validation\n",
    "pieces_tmp = piano_midi_pieces_filtered.copy()\n",
    "#pieces_tmp = chopin_pieces_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "def free_memory():\n",
    "    all_pieces.clear()\n",
    "    chopin_only_pieces.clear()\n",
    "    piano_midi_only_pieces.clear()\n",
    "    piano_midi_pieces_filtered.clear()\n",
    "    piano_midi_pieces_remaining.clear()\n",
    "    pieces_tmp.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Either select one validation and one training piece or \n",
    "# set aside a random set of pieces for validation purposes\n",
    "n_subset = 30\n",
    "#selection = 'single_piece' \n",
    "#selection = 'subset'\n",
    "selection = 'all'\n",
    "\n",
    "random.seed(1337)\n",
    "if selection == 'single_piece':\n",
    "    validation_pieces = copy.deepcopy({'chop2803' : all_pieces['chop2803']})\n",
    "    training_pieces   = copy.deepcopy({'chop2804' : all_pieces['chop2804']})\n",
    "    free_memory()\n",
    "elif selection == 'subset' or selection == 'all':\n",
    "    if selection == 'subset':\n",
    "        training_pieces = copy.deepcopy({k: pieces_tmp[k] for k in random.sample(list(pieces_tmp.keys()), n_subset)})\n",
    "        free_memory()\n",
    "    elif selection == 'all':\n",
    "        training_pieces = pieces_tmp.copy()\n",
    "    num_validation_pieces = len(training_pieces) // 10\n",
    "\n",
    "    validation_pieces={}\n",
    "    for v in range(num_validation_pieces):\n",
    "        index = random.choice(list(training_pieces.keys()))\n",
    "        validation_pieces[index] = training_pieces.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   pieces =  81\n",
      "Number of validation pieces =  8\n"
     ]
    }
   ],
   "source": [
    "print('Number of training   pieces = ', len(training_pieces))    \n",
    "print('Number of validation pieces = ', len(validation_pieces))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that features (X) and lables (y) generation work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions y: (sample_size, num_notes, num_timesteps, play_articulate_velocity) =  (16, 88, 96, 4)\n",
      "Dimensions X: (sample_size, num_notes, num_timesteps, feature_dim             ) =  (16, 88, 96, 108)\n"
     ]
    }
   ],
   "source": [
    "# Generate sample Note State Matrix for dimension measurement and numerical checking purposes\n",
    "\n",
    "y = tf.convert_to_tensor(batch.getPieceBatch(training_pieces, # 16\n",
    "                                             batch_size = 16,\n",
    "                                             num_time_steps = 16*3*2), \n",
    "                         dtype=tf.float32) \n",
    "X = Input_Kernel(y, Midi_low = lowerBound, Midi_high = upperBound - 1)\n",
    "X, y = alignXy(X,y)\n",
    "\n",
    "print('Dimensions y: (sample_size, num_notes, num_timesteps, play_articulate_velocity) = ', y.shape)\n",
    "print('Dimensions X: (sample_size, num_notes, num_timesteps, feature_dim             ) = ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Midi_low = lowerBound\n",
    "Midi_high = upperBound - 1\n",
    "num_notes = Midi_high + 1 - Midi_low # X.shape[1] = Midi_high + 1 - Midi_low \n",
    "num_timesteps = 16*3*2 \n",
    "input_size = 4\n",
    "keep_prob = 0.5\n",
    "\n",
    "num_t_units = [128, 128] # [200, 200]\n",
    "num_n_units = [64, 64] # [100, 100]\n",
    "dense_units = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start building of model graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Graph...\n"
     ]
    }
   ],
   "source": [
    "# Build the Model Graph:\n",
    "tf.reset_default_graph()\n",
    "print('Building Graph...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note_State_Expand shape =  (?, 88, ?, 108)\n",
      "Note_State_Batch shape =  (?, 88, ?, 4)\n"
     ]
    }
   ],
   "source": [
    "# Graph Input Placeholders\n",
    "Note_State_Batch = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, input_size], name= \"Note_State_Batch\")\n",
    "output_keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name= \"output_keep_prob\")\n",
    "\n",
    "#Generate expanded tensor from batch of note state matrices\n",
    "Note_State_Expand = Input_Kernel(Note_State_Batch, \n",
    "                                 Midi_low=Midi_low, \n",
    "                                 Midi_high=Midi_high #,\n",
    "                                 #time_init=time_init\n",
    "                                )\n",
    "Note_State_Expand_aligned, Note_State_Batch_aligned = alignXy(Note_State_Expand, Note_State_Batch)\n",
    "\n",
    "print('Note_State_Expand shape = ', Note_State_Expand.get_shape())\n",
    "print('Note_State_Batch shape = ',  Note_State_Batch.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timewise LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-wise output shape =  (?, 88, ?, 128)\n"
     ]
    }
   ],
   "source": [
    "# Generate initial state (at t=0) placeholder\n",
    "timewise_state=[]\n",
    "for i in range(len(num_t_units)):\n",
    "    timewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]]) #None = batch_size * num_notes\n",
    "    timewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]])\n",
    "    timewise_state.append(LSTMStateTuple(timewise_h, timewise_c))\n",
    "\n",
    "timewise_state=tuple(timewise_state)\n",
    "\n",
    "timewise_cell = LSTM_Cell(num_t_units, output_keep_prob)\n",
    "\n",
    "timewise_out, timewise_state_out = LSTM_Layer(input_data=Note_State_Expand_aligned,\n",
    "                                              state_init=timewise_state,\n",
    "                                              cell = timewise_cell,\n",
    "                                              time_or_note=\"time\")\n",
    "\n",
    "print('Time-wise output shape = ', timewise_out.get_shape())\n",
    "# print('Time-wise state shape = ', timewise_state_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notewise LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note-wise output shape =  (?, 88, ?, 64)\n"
     ]
    }
   ],
   "source": [
    "#LSTM Note Wise Graph\n",
    "\n",
    "# Generate initial state (at n=0) placeholder\n",
    "notewise_state=[]\n",
    "for i in range(len(num_n_units)):\n",
    "    notewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]]) #None = batch_size * num_timesteps\n",
    "    notewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]])\n",
    "    notewise_state.append(LSTMStateTuple(notewise_h, notewise_c))\n",
    "\n",
    "notewise_state=tuple(notewise_state)\n",
    "\n",
    "notewise_cell = LSTM_Cell(num_n_units, output_keep_prob)\n",
    "\n",
    "notewise_out, notewise_state_out =  LSTM_Layer(input_data=timewise_out,\n",
    "                                               state_init=notewise_state,\n",
    "                                               cell=notewise_cell,\n",
    "                                               time_or_note=\"note\")\n",
    "\n",
    "print('Note-wise output shape = ', notewise_out.get_shape())\n",
    "# print('Note-wise state shape = ', notewise_state_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate conditional probabilty using dense layers to generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_articulate_logit output shape =  (?, 88, ?, 2)\n",
      "velocity output shape =  (?, 88, ?, 1)\n",
      "play_articulate_sampled output shape =  (?, 88, ?, 2)\n"
     ]
    }
   ],
   "source": [
    "output_1, output_2, output_3 = Conditional_Probability_Layer(notewise_out, dense_units=dense_units)\n",
    "\n",
    "print('play_articulate_logit output shape = ', output_1.get_shape())\n",
    "print('velocity output shape = ', output_2.get_shape()) \n",
    "print('play_articulate_sampled output shape = ', output_3.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finished building of model graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Building Complete\n"
     ]
    }
   ],
   "source": [
    "print('Graph Building Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "alpha = 0.01 \n",
    "loss_p_a, log_likelihood = Loss_Function_1(Note_State_Batch_aligned, output_1)\n",
    "loss_velocity = Loss_Function_2(Note_State_Batch_aligned, output_2)\n",
    "loss = loss_p_a + alpha * 1 / 127 * tf.sqrt(loss_velocity)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate = 1, epsilon=1e-04).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF1 specific parameters \n",
    "restore_model_name = None\n",
    "save_model_name = 'Long_Train_256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "epochs = 16\n",
    "batch_size = 2\n",
    "epoch_save_list = [1, 2, 4, 8, 16]\n",
    "\n",
    "\n",
    "\n",
    "n_train_batches = getNumberOfBatches(training_pieces, batch_size, num_timesteps)\n",
    "n_val_batches = getNumberOfBatches(validation_pieces, batch_size, num_timesteps)\n",
    "\n",
    "# Values for loss, metric and confusion matrix\n",
    "train_loss_p_a_array   = np.full((epochs, n_train_batches), 10.0)\n",
    "train_loss_vel_array   = np.full((epochs, n_train_batches), 10.0)\n",
    "train_metric_p_a_array = np.full((epochs, n_train_batches), 10.0)\n",
    "train_metric_vel_array = np.full((epochs, n_train_batches), 10.0)\n",
    "val_loss_p_a_array     = np.full((epochs, n_val_batches), 10.0)\n",
    "val_loss_vel_array     = np.full((epochs, n_val_batches), 10.0)\n",
    "val_metric_p_a_array   = np.full((epochs, n_val_batches), 10.0)\n",
    "val_metric_vel_array   = np.full((epochs, n_val_batches), 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4333, 605)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_train_batches, n_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Epoch [1/16]\n",
      "\n",
      "\n",
      "Training batch: 4333/4333\n",
      "Validation batch: 605/605\n",
      "Training   Loss p_a: 0.06812044371336659\n",
      "Validation Loss p_a: 0.045973980568410934\n",
      "Training   Loss vel: 741.1412369472009\n",
      "Validation Loss vel: 629.5551560583193\n",
      "\n",
      "Time: 4312.516s\n",
      "Start of Epoch [2/16]\n",
      "\n",
      "\n",
      "Training batch: 1423/4333\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-73484a5f039b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 l_1, l_2, log_likelihood_run, _, velocity_gen_out_run ,note_gen_out_run, Note_State_Batch_result = sess.run(\n\u001b[1;32m     57\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mloss_p_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_velocity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNote_State_Batch_aligned\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     feed_dict = feed_dict)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mtrain_loss_p_a_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tf1-lyTET9H8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tf1-lyTET9H8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tf1-lyTET9H8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tf1-lyTET9H8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tf1-lyTET9H8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "time_old = start_time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # try to restore the pre_trained\n",
    "    if restore_model_name is not None:\n",
    "        Load_Directory = Checkpoint_Directory + restore_model_name\n",
    "               \n",
    "        print(\"Load the model from: {}\".format(restore_model_name))\n",
    "        saver.restore(sess, Load_Directory + '/{}'.format(restore_model_name))\n",
    "        \n",
    "    \n",
    "    # Initial States\n",
    "    timewise_state_val = []\n",
    "    for i in range(len(num_t_units)):\n",
    "        c_t = np.zeros((batch_size * num_notes, num_t_units[i])) \n",
    "        h_t = np.zeros((batch_size * num_notes, num_t_units[i]))\n",
    "        timewise_state_val.append(LSTMStateTuple(h_t, c_t))\n",
    "        \n",
    "    notewise_state_val = []\n",
    "    for i in range(len(num_n_units)):\n",
    "        c_n = np.zeros((batch_size * num_timesteps, num_n_units[i])) \n",
    "        h_n = np.zeros((batch_size * num_timesteps, num_n_units[i]))\n",
    "        notewise_state_val.append(LSTMStateTuple(h_n, c_n))\n",
    "        \n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        print('\\rStart of Epoch [%d/%d]'% (epoch + 1, epochs))\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        # Generate batch of training data   \n",
    "        n = 0\n",
    "        for k in training_pieces.keys():\n",
    "            start_old = 0\n",
    "            piece = training_pieces[str(k)]\n",
    "            while start_old < (len(piece) - num_timesteps):\n",
    "                print('\\rTraining batch: %d/%d' % (n + 1, n_train_batches), end='\\r')\n",
    "                batch_input_state_train, start_old = batch.getPieceBatch2(piece, \n",
    "                                                                    num_time_steps = num_timesteps, \n",
    "                                                                    batch_size = batch_size,\n",
    "                                                                    start_old = start_old)    \n",
    "\n",
    "            \n",
    "                # Run Session\n",
    "                feed_dict = {Note_State_Batch: batch_input_state_train, \n",
    "                             output_keep_prob: keep_prob, \n",
    "                             timewise_state: timewise_state_val, \n",
    "                             notewise_state: notewise_state_val}\n",
    "\n",
    "\n",
    "                l_1, l_2, log_likelihood_run, _, velocity_gen_out_run ,note_gen_out_run, Note_State_Batch_result = sess.run(\n",
    "                    [loss_p_a, loss_velocity, log_likelihood, optimizer, output_2, output_3, Note_State_Batch_aligned], \n",
    "                    feed_dict = feed_dict)\n",
    "                \n",
    "                train_loss_p_a_array[epoch, n] = l_1\n",
    "                train_loss_vel_array[epoch, n] = l_2\n",
    "                \n",
    "                n += 1\n",
    "        print('')\n",
    "        \n",
    "        # Generate batch of validation data   \n",
    "        n = 0\n",
    "        for k in validation_pieces.keys():\n",
    "            start_old = 0\n",
    "            piece = validation_pieces[str(k)]\n",
    "            while start_old < (len(piece)- num_timesteps):\n",
    "                print('Validation batch: %d/%d' % (n + 1, n_val_batches), end='\\r')\n",
    "                batch_input_state_val, start_old = batch.getPieceBatch2(piece, \n",
    "                                                                    num_time_steps = num_timesteps, \n",
    "                                                                    batch_size = batch_size, \n",
    "                                                                    start_old = start_old)    \n",
    "\n",
    "                # Run Session\n",
    "                feed_dict = {Note_State_Batch: batch_input_state_val, \n",
    "                             output_keep_prob: keep_prob, \n",
    "                             timewise_state: timewise_state_val, \n",
    "                             notewise_state: notewise_state_val}\n",
    "\n",
    "\n",
    "                l_1, l_2, log_likelihood_run, = sess.run(\n",
    "                    [loss_p_a, loss_velocity, log_likelihood], \n",
    "                    feed_dict = feed_dict)\n",
    "\n",
    "                val_loss_p_a_array[epoch, n] = l_1\n",
    "                val_loss_vel_array[epoch, n] = l_2\n",
    "                n += 1\n",
    "        print('')\n",
    "\n",
    "\n",
    "        time_new = time.time()\n",
    "        duration = time_new - time_old\n",
    "        time_old = time_new\n",
    "        print('Training   Loss p_a: '     + str(np.mean(train_loss_p_a_array[epoch,:])))\n",
    "        print('Validation Loss p_a: '     + str(np.mean(val_loss_p_a_array[epoch,:])))\n",
    "        print('Training   Loss vel: '     + str(np.mean(train_loss_vel_array[epoch,:])))\n",
    "        print('Validation Loss vel: '     + str(np.mean(val_loss_vel_array[epoch,:])))\n",
    "        print('')\n",
    "        print('Time: ' +  str(round(duration, 3)) + 's')\n",
    "\n",
    "        # Periodically save model and loss histories\n",
    "        if (epoch + 1) in epoch_save_list:\n",
    "\n",
    "            model_save_path = Checkpoint_Directory + current_time_str[:-7] + '/{}'.format(save_model_name)\n",
    "            np_save_path = Numpy_Directory + current_time_str[:-7] + '/' \n",
    "            save_path = saver.save(sess, model_save_path)            \n",
    "            try:\n",
    "                os.mkdir(np_save_path) \n",
    "            except:\n",
    "                pass\n",
    "            np.savez(np_save_path + save_model_name + '_array', \n",
    "                     train_loss_p_a_array, \n",
    "                     train_loss_vel_array,\n",
    "                     val_loss_p_a_array, \n",
    "                     val_loss_vel_array,\n",
    "                    ) \n",
    "            for i in range(batch_size):\n",
    "                midi.generate_audio(batch_input_state_train[i:(i+1),:,:,:], \n",
    "                                    Music_Out_Train_Directory + current_time_str[:-7] + '/',\n",
    "                                    'train' + '_epoch_' + str(epoch + 1) + '_batch_' + str(i) + '_true', \n",
    "                                    verbose = False)\n",
    "            prediction = np.concatenate([note_gen_out_run, velocity_gen_out_run, Note_State_Batch_result[:,:,:,3:4]], axis=-1)\n",
    "            for i in range(batch_size):\n",
    "                midi.generate_audio(prediction[i:(i+1),:,:,:], \n",
    "                                    Music_Out_Train_Directory + current_time_str[:-7] + '/',\n",
    "                                    'train' + '_epoch_' + str(epoch + 1) + '_batch_' + str(i) + '_predict', \n",
    "                                    verbose = False)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Training time = ', end_time - start_time, ' seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
