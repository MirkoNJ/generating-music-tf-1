{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import pdb\n",
    "import random\n",
    "import time\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third party imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, LSTMStateTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.nikhil.midi_related as midi\n",
    "import modules.nikhil.batch as batch\n",
    "\n",
    "from modules.nikhil.MyFunctions import (\n",
    "    alignXy,\n",
    "    Conditional_Probability_Layer,\n",
    "    Input_Kernel, \n",
    "    getNumberOfBatches,\n",
    "    Loss_Function_1,\n",
    "    Loss_Function_2,\n",
    "    LSTM_Cell,\n",
    "    LSTM_Layer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extensions and autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting relative directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Working_Directory = os.getcwd()\n",
    "Project_Directory = os.path.abspath(os.path.join(Working_Directory,'..'))\n",
    "Music_In_Directory = Project_Directory + \"/data/\" \n",
    "Output_Directory = Project_Directory + \"/outputs/\"\n",
    "Model_Directory = Output_Directory + \"models/\"\n",
    "Music_Out_Directory = Output_Directory + \"midi/\"\n",
    "Music_Out_Train_Directory = Music_Out_Directory + \"train/\"\n",
    "Checkpoint_Directory = Model_Directory + \"ckpt/\"\n",
    "Numpy_Directory = Model_Directory + \"arrays/\"\n",
    "\n",
    "Midi_Directories = [\n",
    "    \"albeniz\", \n",
    "    \"beeth\",\n",
    "    \"borodin\",\n",
    "    \"brahms\",\n",
    "    \"burgm\",\n",
    "    \"chopin\", \n",
    "    #\"chopin_midi\",\n",
    "    \"debussy\", \n",
    "    \"granados\", \n",
    "    \"grieg\", \n",
    "    \"haydn\", \n",
    "    \"liszt\", \n",
    "    \"mendelssohn\", \n",
    "    \"mozart\", \n",
    "    \"muss\", \n",
    "    \"schubert\", \n",
    "    \"schumann\", \n",
    "    \"tschai\"\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pieces (i.e. import midi files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checkt that importing single midi (i.e. Beethoven's Fuer Elise) works\n",
    "elise = midi.midiToNoteStateMatrix(Music_In_Directory + \"beeth/elise.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time_steps = 128 # only files with at least this many 48th note steps are saved\n",
    "lowerBound = 21\n",
    "upperBound = 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded alb_esp1\n",
      "Loaded alb_esp2\n",
      "Loaded alb_esp3\n",
      "Loaded alb_esp4\n",
      "Loaded alb_esp5\n",
      "Loaded alb_se1\n",
      "Loaded alb_se2\n",
      "Loaded alb_se3\n",
      "Loaded alb_se4\n",
      "Loaded alb_se5\n",
      "Loaded alb_se6\n",
      "Loaded alb_se7\n",
      "Loaded alb_se8\n",
      "Loading directory albeniz took 11.884s\n",
      "Loaded appass_1\n",
      "Loaded appass_2\n",
      "Loaded appass_3\n",
      "Loaded beethoven_hammerklavier_1\n",
      "Loaded beethoven_hammerklavier_2\n",
      "Loaded beethoven_hammerklavier_3\n",
      "Loaded beethoven_hammerklavier_4\n",
      "Loaded beethoven_les_adieux_1\n",
      "Loaded beethoven_les_adieux_2\n",
      "Loaded beethoven_les_adieux_3\n",
      "Loaded beethoven_opus10_1\n",
      "Loaded beethoven_opus10_2\n",
      "Loaded beethoven_opus10_3\n",
      "Loaded beethoven_opus22_1\n",
      "Loaded beethoven_opus22_2\n",
      "Loaded beethoven_opus22_3\n",
      "Loaded beethoven_opus22_4\n",
      "Loaded beethoven_opus90_1\n",
      "Loaded beethoven_opus90_2\n",
      "Loaded elise\n",
      "Loaded mond_1\n",
      "Loaded mond_2\n",
      "Loaded mond_3\n",
      "Loaded pathetique_1\n",
      "Loaded pathetique_2\n",
      "Loaded pathetique_3\n",
      "Loaded waldstein_1\n",
      "Loaded waldstein_2\n",
      "Loaded waldstein_3\n",
      "Loading directory beeth took 70.698s\n",
      "Loaded bor_ps1\n",
      "Loaded bor_ps2\n",
      "Loaded bor_ps3\n",
      "Loaded bor_ps4\n",
      "Loaded bor_ps5\n",
      "Loaded bor_ps6\n",
      "Loaded bor_ps7\n",
      "Loading directory borodin took 3.82s\n",
      "Loaded BR_IM6\n",
      "Loaded br_im2\n",
      "Loaded br_im5\n",
      "Loaded br_rhap\n",
      "Loaded brahms_opus117_1\n",
      "Loaded brahms_opus117_2\n",
      "Loaded brahms_opus1_1\n",
      "Loaded brahms_opus1_2\n",
      "Loaded brahms_opus1_3\n",
      "Loaded brahms_opus1_4\n",
      "Loading directory brahms took 21.381s\n",
      "Loaded burg_agitato\n",
      "Loaded burg_erwachen\n",
      "Loaded burg_geschwindigkeit\n",
      "Loaded burg_gewitter\n",
      "Loaded burg_perlen\n",
      "Loaded burg_quelle\n",
      "Loaded burg_spinnerlied\n",
      "Loaded burg_sylphen\n",
      "Loaded burg_trennung\n",
      "Loading directory burgm took 2.542s\n",
      "Loaded chpn-p1\n",
      "Loaded chpn-p10\n",
      "Loaded chpn-p11\n",
      "Loaded chpn-p12\n",
      "Loaded chpn-p13\n",
      "Loaded chpn-p14\n",
      "Loaded chpn-p15\n",
      "Loaded chpn-p16\n",
      "Loaded chpn-p17\n",
      "Loaded chpn-p18\n",
      "Loaded chpn-p19\n",
      "Loaded chpn-p2\n",
      "Loaded chpn-p20\n",
      "Loaded chpn-p21\n",
      "Loaded chpn-p22\n",
      "Loaded chpn-p23\n",
      "Loaded chpn-p24\n",
      "Loaded chpn-p3\n",
      "Loaded chpn-p4\n",
      "Loaded chpn-p5\n",
      "Loaded chpn-p6\n",
      "Loaded chpn-p7\n",
      "Loaded chpn-p8\n",
      "Loaded chpn-p9\n",
      "Loaded chpn_op18\n",
      "Loaded chpn_op23\n",
      "Loaded chpn_op25_e1\n",
      "Loaded chpn_op25_e11\n",
      "Loaded chpn_op25_e12\n",
      "Loaded chpn_op25_e2\n",
      "Loaded chpn_op25_e3\n",
      "Loaded chpn_op25_e4\n",
      "Loaded chpn_op27_1\n",
      "Loaded chpn_op27_2\n",
      "Loaded chpn_op33_2\n",
      "Loaded chpn_op33_4\n",
      "Loaded chpn_op35_1\n",
      "Loaded chpn_op35_2\n",
      "Loaded chpn_op35_3\n",
      "Loaded chpn_op35_4\n",
      "Loaded chpn_op53\n",
      "Loaded chpn_op66\n",
      "Loaded chpn_op7_1\n",
      "Loaded chpn_op7_2\n",
      "Loading directory chopin took 35.894s\n",
      "Loaded DEB_CLAI\n",
      "Loaded DEB_PASS\n",
      "Loaded deb_menu\n",
      "Loaded deb_prel\n",
      "Loaded debussy_cc_1\n",
      "Loaded debussy_cc_2\n",
      "Loaded debussy_cc_3\n",
      "Loaded debussy_cc_4\n",
      "Loaded debussy_cc_6\n",
      "Loading directory debussy took 9.485s\n",
      "Loaded gra_esp_2\n",
      "Loaded gra_esp_3\n",
      "Loaded gra_esp_4\n",
      "Loading directory granados took 3.062s\n",
      "Loaded grieg_album\n",
      "Loaded grieg_berceuse\n",
      "Loaded grieg_brooklet\n",
      "Loaded grieg_butterfly\n",
      "Loaded grieg_elfentanz\n",
      "Loaded grieg_halling\n",
      "Loaded grieg_kobold\n",
      "Loaded grieg_march\n",
      "Loaded grieg_once_upon_a_time\n",
      "Loaded grieg_spring\n",
      "Loaded grieg_voeglein\n",
      "Loaded grieg_waechter\n",
      "Loaded grieg_walzer\n",
      "Loaded grieg_wanderer\n",
      "Loaded grieg_wedding\n",
      "Loaded grieg_zwerge\n",
      "Loading directory grieg took 16.269s\n",
      "Loaded hay_40_1\n",
      "Loaded hay_40_2\n",
      "Loaded haydn_33_1\n",
      "Loaded haydn_33_2\n",
      "Loaded haydn_33_3\n",
      "Loaded haydn_35_1\n",
      "Loaded haydn_35_2\n",
      "Loaded haydn_35_3\n",
      "Loaded haydn_43_1\n",
      "Loaded haydn_43_2\n",
      "Loaded haydn_43_3\n",
      "Loaded haydn_7_1\n",
      "Loaded haydn_7_2\n",
      "Loaded haydn_7_3\n",
      "Loaded haydn_8_1\n",
      "Loaded haydn_8_2\n",
      "Loaded haydn_8_3\n",
      "Loaded haydn_8_4\n",
      "Loaded haydn_9_1\n",
      "Loaded haydn_9_2\n",
      "Loaded haydn_9_3\n",
      "Loading directory haydn took 20.0s\n",
      "Loaded liz_donjuan\n",
      "Loaded liz_et1\n",
      "Loaded liz_et2\n",
      "Loaded liz_et3\n",
      "Loaded liz_et4\n",
      "Loaded liz_et5\n",
      "Loaded liz_et_trans4\n",
      "Loaded liz_et_trans5\n",
      "Loaded liz_et_trans8\n",
      "Loaded liz_liebestraum\n",
      "Loaded liz_rhap02\n",
      "Loaded liz_rhap09\n",
      "Loaded liz_rhap12\n",
      "Loaded liz_rhap15\n",
      "Loading directory liszt took 22.263s\n",
      "Loaded mendel_op19_1\n",
      "Loaded mendel_op19_2\n",
      "Loaded mendel_op19_3\n",
      "Loaded mendel_op19_4\n",
      "Loaded mendel_op19_5\n",
      "Loaded mendel_op19_6\n",
      "Loaded mendel_op30_1\n",
      "Loaded mendel_op30_2\n",
      "Loaded mendel_op30_3\n",
      "Loaded mendel_op30_4\n",
      "Loaded mendel_op30_5\n",
      "Loaded mendel_op53_5\n",
      "Loaded mendel_op62_3\n",
      "Loaded mendel_op62_4\n",
      "Loaded mendel_op62_5\n",
      "Loading directory mendelssohn took 8.028s\n",
      "Loaded mz_311_1\n",
      "Loaded mz_311_2\n",
      "Loaded mz_311_3\n",
      "Loaded mz_330_1\n",
      "Loaded mz_330_2\n",
      "Loaded mz_330_3\n",
      "Loaded mz_331_1\n",
      "Loaded mz_331_2\n",
      "Loaded mz_331_3\n",
      "Loaded mz_332_1\n",
      "Loaded mz_332_2\n",
      "Loaded mz_332_3\n",
      "Loaded mz_333_1\n",
      "Loaded mz_333_2\n",
      "Loaded mz_333_3\n",
      "Loaded mz_545_1\n",
      "Loaded mz_545_2\n",
      "Loaded mz_545_3\n",
      "Loaded mz_570_1\n",
      "Loaded mz_570_2\n",
      "Loaded mz_570_3\n",
      "Loading directory mozart took 60.128s\n",
      "Loaded muss_4\n",
      "Loaded muss_6\n",
      "Loaded muss_8\n",
      "Loading directory muss took 3.872s\n",
      "Loaded schu_143_1\n",
      "Loaded schu_143_2\n",
      "Loaded schu_143_3\n",
      "Loaded schub_d760_1\n",
      "Loaded schub_d760_2\n",
      "Loaded schub_d760_3\n",
      "Loaded schub_d760_4\n",
      "Loaded schub_d960_1\n",
      "Loaded schub_d960_2\n",
      "Loaded schub_d960_3\n",
      "Loaded schub_d960_4\n",
      "Loaded schubert_D850_1\n",
      "Loaded schubert_D850_2\n",
      "Loaded schubert_D850_3\n",
      "Loaded schubert_D850_4\n",
      "Loaded schubert_D935_1\n",
      "Loaded schubert_D935_2\n",
      "Loaded schubert_D935_3\n",
      "Loaded schubert_D935_4\n",
      "Loaded schuim-1\n",
      "Loaded schuim-2\n",
      "Loaded schuim-3\n",
      "Loaded schuim-4\n",
      "Loaded schumm-1\n",
      "Loaded schumm-2\n",
      "Loaded schumm-3\n",
      "Loaded schumm-4\n",
      "Loaded schumm-5\n",
      "Loaded schumm-6\n",
      "Loading directory schubert took 83.175s\n",
      "Loaded schum_abegg\n",
      "Loaded scn15_1\n",
      "Loaded scn15_10\n",
      "Loaded scn15_11\n",
      "Loaded scn15_12\n",
      "Loaded scn15_13\n",
      "Loaded scn15_2\n",
      "Loaded scn15_3\n",
      "Loaded scn15_4\n",
      "Loaded scn15_5\n",
      "Loaded scn15_6\n",
      "Loaded scn15_7\n",
      "Loaded scn15_8\n",
      "Loaded scn15_9\n",
      "Loaded scn16_1\n",
      "Loaded scn16_2\n",
      "Loaded scn16_3\n",
      "Loaded scn16_4\n",
      "Loaded scn16_5\n",
      "Loaded scn16_6\n",
      "Loaded scn16_7\n",
      "Loaded scn16_8\n",
      "Loaded scn68_10\n",
      "Loaded scn68_12\n",
      "Loading directory schumann took 10.84s\n",
      "Loaded ty_april\n",
      "Loaded ty_august\n",
      "Loaded ty_dezember\n",
      "Loaded ty_februar\n",
      "Loaded ty_januar\n",
      "Loaded ty_juli\n",
      "Loaded ty_juni\n",
      "Loaded ty_maerz\n",
      "Loaded ty_mai\n",
      "Loaded ty_november\n",
      "Loaded ty_oktober\n",
      "Loaded ty_september\n",
      "Loading directory tschai took 9.198s\n",
      "Number of total pieces =  279\n",
      "Loading all pieces took 392.546s\n"
     ]
    }
   ],
   "source": [
    "# Import all Midi data\n",
    "\n",
    "\n",
    "all_pieces = {}\n",
    "chopin_only_pieces = {}\n",
    "piano_midi_only_pieces = {}\n",
    "\n",
    "start_time_loading = time.time()\n",
    "time_loading_old = start_time_loading\n",
    "\n",
    "# Gather the pieces from the specified directory\n",
    "for f in range(len(Midi_Directories)):\n",
    "    Training_Midi_Folder = Music_In_Directory + Midi_Directories[f]\n",
    "    if Midi_Directories[f] == 'chopin_midi':\n",
    "        chopin_only_pieces = {**chopin_only_pieces, **midi.loadPieces(Training_Midi_Folder,\n",
    "                                                                      min_time_steps,\n",
    "                                                                      lowerBound, \n",
    "                                                                      upperBound,\n",
    "                                                                      verbose=False,\n",
    "                                                                      verbose_name=True)}\n",
    "    else: \n",
    "        piano_midi_only_pieces = {**piano_midi_only_pieces, **midi.loadPieces(Training_Midi_Folder,\n",
    "                                                                              min_time_steps,\n",
    "                                                                              lowerBound, \n",
    "                                                                              upperBound,\n",
    "                                                                              verbose=False,\n",
    "                                                                              verbose_name=True)}\n",
    "    time_loading_new = time.time()\n",
    "    duration = time_loading_new - time_loading_old\n",
    "    time_loading_old = time_loading_new\n",
    "    print('Loading directory ' + Midi_Directories[f] + ' took ' + str(round(duration, 3)) + 's' )\n",
    "\n",
    "all_pieces = {**chopin_only_pieces, **piano_midi_only_pieces}\n",
    "end_time_loading = time.time()\n",
    "\n",
    "print('Number of total pieces = ', len(all_pieces))    \n",
    "print('Loading all pieces took ' +  str(round(end_time_loading - start_time_loading, 3)) + 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_time_signatures = [16]\n",
    "odd_time_signatures = [12]\n",
    "sel_time_signautres = even_time_signatures\n",
    "#sel_time_signautres = odd_time_signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  'incorrect' Chopin pieces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chopin pieces loaded =  0\n"
     ]
    }
   ],
   "source": [
    "print('Number of Chopin pieces loaded = ', len(chopin_only_pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check time signature occurences\n",
    "time_signatures = []\n",
    "verbose = False #True\n",
    "keys = list(chopin_only_pieces.keys())\n",
    "for k in keys:\n",
    "    piece = chopin_only_pieces[str(k)]\n",
    "    time_signature = max([b[0][3] for b in  piece])\n",
    "    if str(k) in midi.EXACT_FILES:\n",
    "        time_signatures.append(time_signature)\n",
    "    if verbose:\n",
    "        print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))\n",
    "time_signatures = np.array(time_signatures)\n",
    "unique, counts = np.unique(time_signatures, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only include pieces which were not recoreded (i.e. which MIDI files are exact) and are in (2/4 and 4/4) or in (3/4)\n",
    "time_signatures = []\n",
    "verbose = False #True\n",
    "chopin_pieces_filtered = chopin_only_pieces.copy()\n",
    "keys = list(chopin_pieces_filtered.keys())\n",
    "\n",
    "for k in keys:\n",
    "        piece = chopin_pieces_filtered[str(k)]\n",
    "        time_signature = max([b[0][3] for b in  piece])\n",
    "        if not ((time_signature in sel_time_signautres) and str(k) in midi.EXACT_FILES) :\n",
    "            chopin_pieces_filtered.pop(k)\n",
    "        else:\n",
    "            time_signatures.append(time_signature)\n",
    "        if verbose:\n",
    "            print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))\n",
    "time_signatures = np.array(time_signatures)\n",
    "unique, counts = np.unique(time_signatures, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pieces by Chopin left after filtering =  0\n"
     ]
    }
   ],
   "source": [
    "print('Number of pieces by Chopin left after filtering = ', len(chopin_pieces_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Piano Midi pieces by 3/4 or 4/4 measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Piano Midi pieces loaded =  279\n"
     ]
    }
   ],
   "source": [
    "print('Number of Piano Midi pieces loaded = ', len(piano_midi_only_pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 2,\n",
       " 6: 16,\n",
       " 8: 47,\n",
       " 9: 1,\n",
       " 12: 92,\n",
       " 16: 89,\n",
       " 18: 6,\n",
       " 24: 10,\n",
       " 28: 1,\n",
       " 32: 6,\n",
       " 36: 3,\n",
       " 40: 4,\n",
       " 48: 1,\n",
       " 56: 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check time signature occurences\n",
    "time_signatures = []\n",
    "verbose = False #True\n",
    "keys = list(piano_midi_only_pieces.keys())\n",
    "for k in keys:\n",
    "    piece = piano_midi_only_pieces[str(k)]\n",
    "    time_signature = max([b[0][3] for b in  piece])\n",
    "    time_signatures.append(time_signature)\n",
    "    if verbose:\n",
    "        print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))\n",
    "time_signatures = np.array(time_signatures)\n",
    "unique, counts = np.unique(time_signatures, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter training pieces by selected time signature\n",
    "piano_midi_pieces_filtered = piano_midi_only_pieces.copy()\n",
    "piano_midi_pieces_remaining = piano_midi_only_pieces.copy()\n",
    "keys = list(piano_midi_only_pieces.keys())\n",
    "verbose = False\n",
    "\n",
    "for k in keys:\n",
    "    piece = piano_midi_only_pieces[str(k)]\n",
    "    time_signature = int(max([b[0][3] for b in  piece]))\n",
    "    if not (time_signature in sel_time_signautres):\n",
    "        piano_midi_pieces_filtered.pop(k)\n",
    "    else:\n",
    "        piano_midi_pieces_remaining.pop(k)\n",
    "    if verbose:\n",
    "        print(\"Piece: {}\".format(k) + \"  Time signature: {}\".format(time_signature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Piano Midi pieces left after filtering =  89\n"
     ]
    }
   ],
   "source": [
    "print('Number of Piano Midi pieces left after filtering = ', len(piano_midi_pieces_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train/Validation pieces split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pieces relevant for training and validation\n",
    "pieces_tmp = piano_midi_pieces_filtered.copy()\n",
    "#pieces_tmp = chopin_pieces_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "def free_memory():\n",
    "    all_pieces.clear()\n",
    "    chopin_only_pieces.clear()\n",
    "    piano_midi_only_pieces.clear()\n",
    "    piano_midi_pieces_filtered.clear()\n",
    "    piano_midi_pieces_remaining.clear()\n",
    "    pieces_tmp.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Either select one validation and one training piece or \n",
    "# set aside a random set of pieces for validation purposes\n",
    "n_subset = 30\n",
    "#selection = 'single_piece' \n",
    "#selection = 'subset'\n",
    "selection = 'all'\n",
    "\n",
    "random.seed(1337)\n",
    "if selection == 'single_piece':\n",
    "    validation_pieces = copy.deepcopy({'chop2803' : all_pieces['chop2803']})\n",
    "    training_pieces   = copy.deepcopy({'chop2804' : all_pieces['chop2804']})\n",
    "    free_memory()\n",
    "elif selection == 'subset' or selection == 'all':\n",
    "    if selection == 'subset':\n",
    "        training_pieces = copy.deepcopy({k: pieces_tmp[k] for k in random.sample(list(pieces_tmp.keys()), n_subset)})\n",
    "        free_memory()\n",
    "    elif selection == 'all':\n",
    "        training_pieces = pieces_tmp.copy()\n",
    "    num_validation_pieces = len(training_pieces) // 10\n",
    "\n",
    "    validation_pieces={}\n",
    "    for v in range(num_validation_pieces):\n",
    "        index = random.choice(list(training_pieces.keys()))\n",
    "        validation_pieces[index] = training_pieces.pop(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training   pieces =  81\n",
      "Number of validation pieces =  8\n"
     ]
    }
   ],
   "source": [
    "print('Number of training   pieces = ', len(training_pieces))    \n",
    "print('Number of validation pieces = ', len(validation_pieces))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check that features (X) and lables (y) generation work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions y: (sample_size, num_notes, num_timesteps, play_articulate_velocity) =  (16, 88, 96, 4)\n",
      "Dimensions X: (sample_size, num_notes, num_timesteps, feature_dim             ) =  (16, 88, 96, 108)\n"
     ]
    }
   ],
   "source": [
    "# Generate sample Note State Matrix for dimension measurement and numerical checking purposes\n",
    "\n",
    "y = tf.convert_to_tensor(batch.getPieceBatch(training_pieces, # 16\n",
    "                                             batch_size = 16,\n",
    "                                             num_time_steps = 16*3*2), \n",
    "                         dtype=tf.float32) \n",
    "X = Input_Kernel(y, Midi_low = lowerBound, Midi_high = upperBound - 1)\n",
    "X, y = alignXy(X,y)\n",
    "\n",
    "print('Dimensions y: (sample_size, num_notes, num_timesteps, play_articulate_velocity) = ', y.shape)\n",
    "print('Dimensions X: (sample_size, num_notes, num_timesteps, feature_dim             ) = ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Midi_low = lowerBound\n",
    "Midi_high = upperBound - 1\n",
    "num_notes = Midi_high + 1 - Midi_low # X.shape[1] = Midi_high + 1 - Midi_low \n",
    "num_timesteps = 16*3*2 \n",
    "input_size = 4\n",
    "keep_prob = 0.5\n",
    "\n",
    "num_t_units = [128, 128] # [200, 200]\n",
    "num_n_units = [64, 64] # [100, 100]\n",
    "dense_units = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start building of model graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Graph...\n"
     ]
    }
   ],
   "source": [
    "# Build the Model Graph:\n",
    "tf.reset_default_graph()\n",
    "print('Building Graph...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note_State_Expand shape =  (?, 88, ?, 108)\n",
      "Note_State_Batch shape =  (?, 88, ?, 4)\n"
     ]
    }
   ],
   "source": [
    "# Graph Input Placeholders\n",
    "Note_State_Batch = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, input_size], name= \"Note_State_Batch\")\n",
    "output_keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name= \"output_keep_prob\")\n",
    "\n",
    "#Generate expanded tensor from batch of note state matrices\n",
    "Note_State_Expand = Input_Kernel(Note_State_Batch, \n",
    "                                 Midi_low=Midi_low, \n",
    "                                 Midi_high=Midi_high #,\n",
    "                                 #time_init=time_init\n",
    "                                )\n",
    "Note_State_Expand_aligned, Note_State_Batch_aligned = alignXy(Note_State_Expand, Note_State_Batch)\n",
    "\n",
    "print('Note_State_Expand shape = ', Note_State_Expand.get_shape())\n",
    "print('Note_State_Batch shape = ',  Note_State_Batch.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timewise LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-wise output shape =  (?, 88, ?, 128)\n"
     ]
    }
   ],
   "source": [
    "# Generate initial state (at t=0) placeholder\n",
    "timewise_state=[]\n",
    "for i in range(len(num_t_units)):\n",
    "    timewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]]) #None = batch_size * num_notes\n",
    "    timewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]])\n",
    "    timewise_state.append(LSTMStateTuple(timewise_h, timewise_c))\n",
    "\n",
    "timewise_state=tuple(timewise_state)\n",
    "\n",
    "timewise_cell = LSTM_Cell(num_t_units, output_keep_prob)\n",
    "\n",
    "timewise_out, timewise_state_out = LSTM_Layer(input_data=Note_State_Expand_aligned,\n",
    "                                              state_init=timewise_state,\n",
    "                                              cell = timewise_cell,\n",
    "                                              time_or_note=\"time\")\n",
    "\n",
    "print('Time-wise output shape = ', timewise_out.get_shape())\n",
    "# print('Time-wise state shape = ', timewise_state_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notewise LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note-wise output shape =  (?, 88, ?, 64)\n"
     ]
    }
   ],
   "source": [
    "#LSTM Note Wise Graph\n",
    "\n",
    "# Generate initial state (at n=0) placeholder\n",
    "notewise_state=[]\n",
    "for i in range(len(num_n_units)):\n",
    "    notewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]]) #None = batch_size * num_timesteps\n",
    "    notewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]])\n",
    "    notewise_state.append(LSTMStateTuple(notewise_h, notewise_c))\n",
    "\n",
    "notewise_state=tuple(notewise_state)\n",
    "\n",
    "notewise_cell = LSTM_Cell(num_n_units, output_keep_prob)\n",
    "\n",
    "notewise_out, notewise_state_out =  LSTM_Layer(input_data=timewise_out,\n",
    "                                               state_init=notewise_state,\n",
    "                                               cell=notewise_cell,\n",
    "                                               time_or_note=\"note\")\n",
    "\n",
    "print('Note-wise output shape = ', notewise_out.get_shape())\n",
    "# print('Note-wise state shape = ', notewise_state_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate conditional probabilty using dense layers to generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play_articulate_logit output shape =  (?, 88, ?, 2)\n",
      "velocity output shape =  (?, 88, ?, 1)\n",
      "play_articulate_sampled output shape =  (?, 88, ?, 2)\n"
     ]
    }
   ],
   "source": [
    "output_1, output_2, output_3 = Conditional_Probability_Layer(notewise_out, dense_units=dense_units)\n",
    "\n",
    "print('play_articulate_logit output shape = ', output_1.get_shape())\n",
    "print('velocity output shape = ', output_2.get_shape()) \n",
    "print('play_articulate_sampled output shape = ', output_3.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finished building of model graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Building Complete\n"
     ]
    }
   ],
   "source": [
    "print('Graph Building Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "alpha = 0.01 \n",
    "loss_p_a, log_likelihood = Loss_Function_1(Note_State_Batch_aligned, output_1)\n",
    "loss_velocity = Loss_Function_2(Note_State_Batch_aligned, output_2)\n",
    "loss = loss_p_a + alpha * 1 / 127 * tf.sqrt(loss_velocity)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate = 1, epsilon=1e-04).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF1 specific parameters \n",
    "restore_model_name = 'Long_Train_256'\n",
    "save_model_name = 'Long_Train_256_plus_chopin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "epochs = 16\n",
    "batch_size = 2\n",
    "epoch_save_list = [1, 2, 4, 8, 16]\n",
    "\n",
    "\n",
    "\n",
    "n_train_batches = getNumberOfBatches(training_pieces, batch_size, num_timesteps)\n",
    "n_val_batches = getNumberOfBatches(validation_pieces, batch_size, num_timesteps)\n",
    "\n",
    "# Values for loss, metric and confusion matrix\n",
    "train_loss_p_a_array   = np.full((epochs, n_train_batches), 10.0)\n",
    "train_loss_vel_array   = np.full((epochs, n_train_batches), 10.0)\n",
    "train_metric_p_a_array = np.full((epochs, n_train_batches), 10.0)\n",
    "train_metric_vel_array = np.full((epochs, n_train_batches), 10.0)\n",
    "val_loss_p_a_array     = np.full((epochs, n_val_batches), 10.0)\n",
    "val_loss_vel_array     = np.full((epochs, n_val_batches), 10.0)\n",
    "val_metric_p_a_array   = np.full((epochs, n_val_batches), 10.0)\n",
    "val_metric_vel_array   = np.full((epochs, n_val_batches), 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889, 56)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_train_batches, n_val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: Long_Train_256\n",
      "INFO:tensorflow:Restoring parameters from /home/mirko/Documents/FHWN/MA/master_thesis/code/tf1/outputs/models/ckpt/20210924/Long_Train_256/Long_Train_256\n",
      "Start of Epoch [1/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.03239194995171576\n",
      "Validation Loss p_a: 0.03505189804958978\n",
      "Training   Loss vel: 473.43345810296967\n",
      "Validation Loss vel: 291.2471008300781\n",
      "\n",
      "Time: 782.664s\n",
      "Start of Epoch [2/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.03113548939207441\n",
      "Validation Loss p_a: 0.034764644323981235\n",
      "Training   Loss vel: 460.3421580826338\n",
      "Validation Loss vel: 295.2358407974243\n",
      "\n",
      "Time: 773.655s\n",
      "Start of Epoch [3/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.030519284666762073\n",
      "Validation Loss p_a: 0.034619370342365334\n",
      "Training   Loss vel: 453.4327190901217\n",
      "Validation Loss vel: 293.3595511572702\n",
      "\n",
      "Time: 772.218s\n",
      "Start of Epoch [4/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.030020218438922486\n",
      "Validation Loss p_a: 0.034620764232905846\n",
      "Training   Loss vel: 447.1762069170065\n",
      "Validation Loss vel: 287.9533168247768\n",
      "\n",
      "Time: 770.943s\n",
      "Start of Epoch [5/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.029595122972756824\n",
      "Validation Loss p_a: 0.03520389227196574\n",
      "Training   Loss vel: 443.7130176986162\n",
      "Validation Loss vel: 293.12220873151506\n",
      "\n",
      "Time: 772.667s\n",
      "Start of Epoch [6/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.029267263017318246\n",
      "Validation Loss p_a: 0.035106344580916424\n",
      "Training   Loss vel: 439.9775883617766\n",
      "Validation Loss vel: 291.6728835787092\n",
      "\n",
      "Time: 771.666s\n",
      "Start of Epoch [7/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.028971260044769843\n",
      "Validation Loss p_a: 0.034942216805315444\n",
      "Training   Loss vel: 438.0082118588974\n",
      "Validation Loss vel: 292.0456502096994\n",
      "\n",
      "Time: 771.863s\n",
      "Start of Epoch [8/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.028718408279925483\n",
      "Validation Loss p_a: 0.03553913613515241\n",
      "Training   Loss vel: 434.7934837727498\n",
      "Validation Loss vel: 294.53665501730785\n",
      "\n",
      "Time: 771.867s\n",
      "Start of Epoch [9/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.028409787482520224\n",
      "Validation Loss p_a: 0.035625158343464136\n",
      "Training   Loss vel: 432.12365713216053\n",
      "Validation Loss vel: 293.5319615772792\n",
      "\n",
      "Time: 772.874s\n",
      "Start of Epoch [10/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.028174895945102547\n",
      "Validation Loss p_a: 0.03535202641173133\n",
      "Training   Loss vel: 430.65872826163223\n",
      "Validation Loss vel: 293.714364188058\n",
      "\n",
      "Time: 771.398s\n",
      "Start of Epoch [11/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.027914337910234056\n",
      "Validation Loss p_a: 0.03552250035240182\n",
      "Training   Loss vel: 429.054436734074\n",
      "Validation Loss vel: 291.3425906045096\n",
      "\n",
      "Time: 771.932s\n",
      "Start of Epoch [12/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.027703627930373724\n",
      "Validation Loss p_a: 0.035481433650212627\n",
      "Training   Loss vel: 427.81413550103343\n",
      "Validation Loss vel: 293.0700190407889\n",
      "\n",
      "Time: 771.939s\n",
      "Start of Epoch [13/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.02753307057033357\n",
      "Validation Loss p_a: 0.03522243786470166\n",
      "Training   Loss vel: 425.0607373609854\n",
      "Validation Loss vel: 295.78246416364397\n",
      "\n",
      "Time: 772.108s\n",
      "Start of Epoch [14/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.027281129233543294\n",
      "Validation Loss p_a: 0.03561067837290466\n",
      "Training   Loss vel: 424.35650258015994\n",
      "Validation Loss vel: 293.8117355619158\n",
      "\n",
      "Time: 772.097s\n",
      "Start of Epoch [15/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.027107398821348983\n",
      "Validation Loss p_a: 0.035444154404103756\n",
      "Training   Loss vel: 423.3371689349111\n",
      "Validation Loss vel: 293.6569378716605\n",
      "\n",
      "Time: 771.962s\n",
      "Start of Epoch [16/16]\n",
      "\n",
      "\n",
      "Training batch: 889/889\n",
      "Validation batch: 56/56\n",
      "Training   Loss p_a: 0.026963443514713924\n",
      "Validation Loss p_a: 0.03610945532896689\n",
      "Training   Loss vel: 422.37519170891983\n",
      "Validation Loss vel: 292.9730012076242\n",
      "\n",
      "Time: 771.932s\n",
      "Training time =  12364.836466550827  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "time_old = start_time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # try to restore the pre_trained\n",
    "    if restore_model_name is not None:\n",
    "        Load_Directory = Checkpoint_Directory + '20210924/' + restore_model_name\n",
    "               \n",
    "        print(\"Load the model from: {}\".format(restore_model_name))\n",
    "        saver.restore(sess, Load_Directory + '/{}'.format(restore_model_name))\n",
    "        \n",
    "    \n",
    "    # Initial States\n",
    "    timewise_state_val = []\n",
    "    for i in range(len(num_t_units)):\n",
    "        c_t = np.zeros((batch_size * num_notes, num_t_units[i])) \n",
    "        h_t = np.zeros((batch_size * num_notes, num_t_units[i]))\n",
    "        timewise_state_val.append(LSTMStateTuple(h_t, c_t))\n",
    "        \n",
    "    notewise_state_val = []\n",
    "    for i in range(len(num_n_units)):\n",
    "        c_n = np.zeros((batch_size * num_timesteps, num_n_units[i])) \n",
    "        h_n = np.zeros((batch_size * num_timesteps, num_n_units[i]))\n",
    "        notewise_state_val.append(LSTMStateTuple(h_n, c_n))\n",
    "        \n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        print('\\rStart of Epoch [%d/%d]'% (epoch + 1, epochs))\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        # Generate batch of training data   \n",
    "        n = 0\n",
    "        for k in training_pieces.keys():\n",
    "            start_old = 0\n",
    "            piece = training_pieces[str(k)]\n",
    "            while start_old < (len(piece) - num_timesteps):\n",
    "                print('\\rTraining batch: %d/%d' % (n + 1, n_train_batches), end='\\r')\n",
    "                batch_input_state_train, start_old = batch.getPieceBatch2(piece, \n",
    "                                                                    num_time_steps = num_timesteps, \n",
    "                                                                    batch_size = batch_size,\n",
    "                                                                    start_old = start_old)    \n",
    "\n",
    "            \n",
    "                # Run Session\n",
    "                feed_dict = {Note_State_Batch: batch_input_state_train, \n",
    "                             output_keep_prob: keep_prob, \n",
    "                             timewise_state: timewise_state_val, \n",
    "                             notewise_state: notewise_state_val}\n",
    "\n",
    "\n",
    "                l_1, l_2, log_likelihood_run, _, velocity_gen_out_run ,note_gen_out_run, Note_State_Batch_result = sess.run(\n",
    "                    [loss_p_a, loss_velocity, log_likelihood, optimizer, output_2, output_3, Note_State_Batch_aligned], \n",
    "                    feed_dict = feed_dict)\n",
    "                \n",
    "                train_loss_p_a_array[epoch, n] = l_1\n",
    "                train_loss_vel_array[epoch, n] = l_2\n",
    "                \n",
    "                n += 1\n",
    "        print('')\n",
    "        \n",
    "        # Generate batch of validation data   \n",
    "        n = 0\n",
    "        for k in validation_pieces.keys():\n",
    "            start_old = 0\n",
    "            piece = validation_pieces[str(k)]\n",
    "            while start_old < (len(piece)- num_timesteps):\n",
    "                print('Validation batch: %d/%d' % (n + 1, n_val_batches), end='\\r')\n",
    "                batch_input_state_val, start_old = batch.getPieceBatch2(piece, \n",
    "                                                                    num_time_steps = num_timesteps, \n",
    "                                                                    batch_size = batch_size, \n",
    "                                                                    start_old = start_old)    \n",
    "\n",
    "                # Run Session\n",
    "                feed_dict = {Note_State_Batch: batch_input_state_val, \n",
    "                             output_keep_prob: keep_prob, \n",
    "                             timewise_state: timewise_state_val, \n",
    "                             notewise_state: notewise_state_val}\n",
    "\n",
    "\n",
    "                l_1, l_2, log_likelihood_run, = sess.run(\n",
    "                    [loss_p_a, loss_velocity, log_likelihood], \n",
    "                    feed_dict = feed_dict)\n",
    "\n",
    "                val_loss_p_a_array[epoch, n] = l_1\n",
    "                val_loss_vel_array[epoch, n] = l_2\n",
    "                n += 1\n",
    "        print('')\n",
    "\n",
    "\n",
    "        time_new = time.time()\n",
    "        duration = time_new - time_old\n",
    "        time_old = time_new\n",
    "        print('Training   Loss p_a: '     + str(np.mean(train_loss_p_a_array[epoch,:])))\n",
    "        print('Validation Loss p_a: '     + str(np.mean(val_loss_p_a_array[epoch,:])))\n",
    "        print('Training   Loss vel: '     + str(np.mean(train_loss_vel_array[epoch,:])))\n",
    "        print('Validation Loss vel: '     + str(np.mean(val_loss_vel_array[epoch,:])))\n",
    "        print('')\n",
    "        print('Time: ' +  str(round(duration, 3)) + 's')\n",
    "\n",
    "        # Periodically save model and loss histories\n",
    "        if (epoch + 1) in epoch_save_list:\n",
    "\n",
    "            model_save_path = Checkpoint_Directory + current_time_str[:-7] + '/{}'.format(save_model_name)\n",
    "            np_save_path = Numpy_Directory + current_time_str[:-7] + '/' \n",
    "            save_path = saver.save(sess, model_save_path)            \n",
    "            try:\n",
    "                os.mkdir(np_save_path) \n",
    "            except:\n",
    "                pass\n",
    "            np.savez(np_save_path + save_model_name + '_array', \n",
    "                     train_loss_p_a_array, \n",
    "                     train_loss_vel_array,\n",
    "                     val_loss_p_a_array, \n",
    "                     val_loss_vel_array,\n",
    "                    ) \n",
    "            for i in range(batch_size):\n",
    "                midi.generate_audio(batch_input_state_train[i:(i+1),:,:,:], \n",
    "                                    Music_Out_Train_Directory + current_time_str[:-7] + '/',\n",
    "                                    'train' + '_epoch_' + str(epoch + 1) + '_batch_' + str(i) + '_true', \n",
    "                                    verbose = False)\n",
    "            prediction = np.concatenate([note_gen_out_run, velocity_gen_out_run, Note_State_Batch_result[:,:,:,3:4]], axis=-1)\n",
    "            for i in range(batch_size):\n",
    "                midi.generate_audio(prediction[i:(i+1),:,:,:], \n",
    "                                    Music_Out_Train_Directory + current_time_str[:-7] + '/',\n",
    "                                    'train' + '_epoch_' + str(epoch + 1) + '_batch_' + str(i) + '_predict', \n",
    "                                    verbose = False)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Training time = ', end_time - start_time, ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 10000\n",
    "batch_size = 2\n",
    "keep_prob = 1\n",
    "train_loss_p_a_array2 = np.full(trials, 10.0)\n",
    "train_log_likelihood  = np.full(trials, 10.0)\n",
    "val_loss_p_a_array2   = np.full(trials, 10.0)\n",
    "val_log_likelihood    = np.full(trials, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: Long_Train_256\n",
      "INFO:tensorflow:Restoring parameters from /home/mirko/Documents/FHWN/MA/master_thesis/code/tf1/outputs/models/ckpt/20210924/Long_Train_256/Long_Train_256\n",
      "Training time =  6321.3224585056305  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()    \n",
    "    Load_Directory = Checkpoint_Directory + '20210924/' + restore_model_name\n",
    "    print(\"Load the model from: {}\".format(restore_model_name))\n",
    "    saver.restore(sess, Load_Directory + '/{}'.format(restore_model_name))\n",
    "\n",
    "    # Initial States\n",
    "    timewise_state_val = []\n",
    "    for i in range(len(num_t_units)):\n",
    "        c_t = np.zeros((batch_size * num_notes, num_t_units[i])) \n",
    "        h_t = np.zeros((batch_size * num_notes, num_t_units[i]))\n",
    "        timewise_state_val.append(LSTMStateTuple(h_t, c_t))\n",
    "        \n",
    "    notewise_state_val = []\n",
    "    for i in range(len(num_n_units)):\n",
    "        c_n = np.zeros((batch_size * num_timesteps, num_n_units[i])) \n",
    "        h_n = np.zeros((batch_size * num_timesteps, num_n_units[i]))\n",
    "        notewise_state_val.append(LSTMStateTuple(h_n, c_n))\n",
    "        \n",
    "    # Training Loop\n",
    "    for trial in range(trials):\n",
    "        print('Start of Trial [%d/%d]'% (trial + 1, trials), end='\\r')\n",
    "\n",
    "        k = random.choice(list(training_pieces.keys()))\n",
    "        piece = training_pieces[str(k)]\n",
    "        sixteenth_index = [b[0][3] for b in  piece]\n",
    "        num_time_steps =  max(sixteenth_index)*3*2 \n",
    "        piece = {k: training_pieces[str(k)]}  \n",
    "        batch_input_state_train = batch.getPieceBatch(piece,\n",
    "                                                      num_time_steps = num_time_steps,\n",
    "                                                      batch_size = batch_size)    \n",
    "\n",
    "        # Run Session\n",
    "        feed_dict = {Note_State_Batch: batch_input_state_train, \n",
    "                     output_keep_prob: keep_prob, \n",
    "                     timewise_state: timewise_state_val, \n",
    "                     notewise_state: notewise_state_val}\n",
    "\n",
    "        l_1, = sess.run([loss_p_a], feed_dict = feed_dict)\n",
    "                \n",
    "        train_loss_p_a_array2[trial] = l_1\n",
    "        train_log_likelihood[trial]  = - num_notes * l_1\n",
    "        \n",
    "        k = random.choice(list(validation_pieces.keys()))\n",
    "        piece = validation_pieces[str(k)]\n",
    "        sixteenth_index = [b[0][3] for b in  piece]\n",
    "        num_time_steps =  max(sixteenth_index)*3*2 \n",
    "        piece = {k: validation_pieces[str(k)]}  \n",
    "        batch_input_state_val = batch.getPieceBatch(piece,\n",
    "                                                    num_time_steps = num_time_steps,\n",
    "                                                    batch_size = batch_size)    \n",
    "            \n",
    "        # Run Session\n",
    "        feed_dict = {Note_State_Batch: batch_input_state_val, \n",
    "                     output_keep_prob: keep_prob, \n",
    "                     timewise_state: timewise_state_val, \n",
    "                     notewise_state: notewise_state_val}\n",
    "\n",
    "        l_1, = sess.run([loss_p_a], feed_dict = feed_dict)\n",
    "                \n",
    "        val_loss_p_a_array2[trial] = l_1\n",
    "        val_log_likelihood[trial]  = - num_notes * l_1\n",
    "        \n",
    "end_time = time.time()\n",
    "print('Training time = ', end_time - start_time, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.15, -2.14)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(np.max(train_log_likelihood),2),  np.around(np.median(train_log_likelihood),2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
